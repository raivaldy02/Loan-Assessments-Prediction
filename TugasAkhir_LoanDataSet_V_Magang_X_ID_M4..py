# -*- coding: utf-8 -*-
"""TugasAkhir_LoanDataSet_V-Magang X/ID M4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SA3pRsn4502DYAiy5bL5K9KnE2pElsS_

#IMPORT LIBRARY DAN LOAD DATA
"""

!wget https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/idx-partners/loan_data_2007_2014.csv

import pandas as pd, numpy as  np, seaborn as sns, warnings,matplotlib.pyplot as plt
import re
warnings.filterwarnings("ignore")

backupLoan =  pd.read_csv("loan_data_2007_2014.csv", low_memory=False)
loanData = backupLoan.copy()

loanData.head()

"""**LOAN MAPPING GOOD OR BAD LOAN**"""

loanData.loan_status.unique()

good_loan = ['Fully Paid','Current','In Grace Period','Does not meet the credit policy. Status:Fully Paid']

loanData['mapping_loan_status'] = np.where(loanData['loan_status'].isin(good_loan), 1,0)

plt.title('Good (1) vs Bad (0) Loans Balance')
sns.barplot(x=loanData.mapping_loan_status.value_counts().index,y=loanData.mapping_loan_status.value_counts().values)

def get_percent(s) : 
  return int(re.search('[0-9]+',s).group()) / 100

def get_dropcol(iterable) : 
  for x in iterable : 
    for y in x : 
      drop_col.append(y)

nan_data = loanData.isnull().mean()
drop_col = [
    'Unnamed: 0',
    'id', 
    'member_id', 
    'url', 
    'title', 
    'desc', 
    'zip_code',
    'emp_title', 
    'sub_grade', 
    'next_pymnt_d', 
    'recoveries', 
    'collection_recovery_fee', 
    'total_rec_prncp' ,
    'total_rec_late_fee',
    'addr_state',
    'application_type',
    'policy_code',
    'issue_d', 
    'loan_status', 
    'pymnt_plan', 
    'out_prncp', 
    'out_prncp_inv', 
    'total_pymnt', 
    'total_pymnt_inv', 
    'total_rec_prncp', 
    'total_rec_int', 
    'total_rec_late_fee', 
    'recoveries', 
    'collection_recovery_fee',  
    'last_pymnt_d',
    'last_pymnt_amnt', 
    'next_pymnt_d'
]


execution_list = [
    nan_data[nan_data > get_percent("50%")].to_dict().keys(), 
    loanData.nunique()[loanData.nunique() < 2].to_dict().keys()
]

get_dropcol(execution_list)

drop_col = list(dict.fromkeys(drop_col))


loanData.drop(columns=drop_col, inplace=True, axis=1)
loanData.dropna(inplace=True)

#Periksa Korelasi
plt.figure(figsize=(24,24))
sns.heatmap(loanData.corr(), annot=True, annot_kws={'size':14})

loanData[['loan_amnt','funded_amnt','funded_amnt_inv']].describe()

loanData.drop(columns = ['funded_amnt', 'funded_amnt_inv'], inplace = True)

loanData.reset_index(drop= True, inplace = True)

loanData.columns

for x in loanData.select_dtypes(include=['object']).columns : 
  print(x)
  print(loanData[x].unique())
  print()

#sebelum konversi dari data kategorik ke numerik
loanData['emp_length'].unique()

cond_tobe_applied = (loanData['emp_length'].astype(str) != 'nan') & (loanData['emp_length'].astype(str) != '< 1 year')

new_result = {
    'true' : loanData['emp_length'].str.replace(r'[^0-9]+',' ',regex=True),
    'false' : '0'
}

loanData['emp_length'] = np.where( cond_tobe_applied ,new_result['true'],new_result['false']).astype('int')

#setelah konversi dari data kategorik ke numerik
loanData['emp_length'].unique()

loanData['term'].unique()

loanData['term'] = loanData['term'].str.replace(r'[^0-9]+','',regex=True).astype('int').map(lambda a : int(a / 12))

loanData['term'].unique()

loanData['earliest_cr_line'].unique()

def to_date(df, column) : 
  today_date = pd.to_datetime(pd.to_datetime('today').strftime("%Y-%m-%d"))
  loanData[column] = pd.to_datetime(df[column], format = "%b-%y")

  df['mnths_since-' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
  df['mnths_since-' + column] = df['mnths_since-' + column].apply(lambda x: df['mnths_since-' + column].max() if x < 0 else x)
  df.drop(columns = [column], inplace = True)

pattern = '(?:(?:Jan|Feb)(?:uary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)'
date_col = [x for x in loanData.select_dtypes(include=['object']).columns if loanData[x].str.contains(pattern).sum()]
for col in date_col : 
  to_date(loanData,col)

loanData.isnull().sum()

loanData.isnull().sum()[loanData.isnull().sum() > 0]

"""

```
# This is formatted as code
```

DATA EXPLORATORY"""

def risk_percentage(x):
    ratio = (loanData.groupby(x)['mapping_loan_status'] # group by
         .value_counts(normalize=True) # calculate the ratio
         .mul(100) # multiply by 100 to be percent
         .rename('risky (%)') # rename column as percent
         .reset_index())
    sns.lineplot(data=ratio[ratio['mapping_loan_status'] == 0], x=x, y='risky (%)')
    plt.title(x)
    plt.show()

unique_cols = [ x for x in loanData.nunique()[loanData.nunique() < 12].sort_values().index if x != 'mapping_loan_status'] + ['mnths_since-earliest_cr_line', 'mnths_since-last_credit_pull_d']
for cols in unique_cols:
    risk_percentage(cols)

plt.figure(figsize=(24,24))
sns.heatmap(loanData.corr(), annot=True, annot_kws={'size':14})

"""**One Hot Encoding**"""

# Konversi kolom kategorik dengan One Hot Encoding
from sklearn.preprocessing import OneHotEncoder
cat_cols = [col for col in loanData.select_dtypes(include='object').columns.tolist()]
onehot_cols = pd.get_dummies(loanData[cat_cols], drop_first=True)

onehot_cols

"""**Standardization**

Standarisasi semua kolom numerik dengan menggunakan fungsi StandardScaler yang disediakan oleh sklearn
"""

from sklearn.preprocessing import StandardScaler

num_cols = [col for col in loanData.columns.tolist() if col not in cat_cols + ['mapping_loan_status']]
ss = StandardScaler()
std_cols = pd.DataFrame(ss.fit_transform(loanData[num_cols]), columns=num_cols)

std_cols

"""**Get Final Data**"""

final_data = pd.concat([onehot_cols, std_cols, loanData[['mapping_loan_status']]], axis=1)
final_data.head()

"""**Model Training and Prediction**

**Data Splitting**

Membuat data train dan data test dengan menggunakan data splitting dengan ketentuan rasio 70/30
"""

X = final_data.drop('mapping_loan_status', axis = 1)
y = final_data['mapping_loan_status']

#spliting data menjadi data train dan juga data test 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, 
                                                    random_state=42,stratify=y)

X_train.shape, X_test.shape

#memeriksa jika label kelas-kelas tidak terdapat imbalance

plt.title('Good (1) vs Bad (0) Loans Balance')
sns.barplot(x = final_data.mapping_loan_status.value_counts().index, y = final_data.mapping_loan_status.value_counts().values)

#memeriksa  imbalance untuk data train
y_train.value_counts()

"""Berdasarkan Ilustrasi *Chart* dan Data diatas, data yang terklasifikasi sebagai **status pinjaman buruk** memiliki tingkat perbandingan yang jauh lebih sedikit jumlahnya dengan **status pinjaman baik** oleh karena nya data set ini dapat di nyatakan sebagai data yang *imbalance*

Melatih model tanpa menangani distribusi kelas *imbalance* menggunakan regresi logistik
"""

# Import Library
from sklearn.linear_model import LogisticRegression  
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve

# training
LR= LogisticRegression(max_iter=600).fit(X_train, y_train)
# predicting
y_pred_LR = LR.predict(X_test)

# classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_LR, digits=2, target_names = target_names))

"""hasil prediksi memiliki hasil yang sangat tidak sama antara kelas=kelas *bad loan* dan *good loan* . dimana untuk kelas *bad loan*, hasil terhadap nilai recall sangat dekat dengan batas nilai kosong / 0. sementara pada kelas *good loan* , hasil terhadap nilai recall mecapai nilai sempurna / 100. Untuk mendapatkan nilai Recall dengan cara angka prediksi bernilai **POSITIF** benar (True Positive) dibagi dengan jumlah angka **POSITIF**. yang berarti model dengan benar mengidentifikasi 2 % dari total *bad loans* dan juga dengan benar mengidentifikasi 100% dari total *good loans*.

model ini mendapatkan akurasi yang cukup tinggi hanya dengan memprediksikan the kelas mayoritas, sedangkan model ini gagal untuk mendapatkan hasil nya untuk kelas minoritas,

hal ini terjadi dikarenakan karena dataset yang *imbalance* sehingga model *machine learning* mengabaikan kelas minoritas atau bad loan secara keseluruhan

jadi, kelas yang *imbalance* data dapat mempengaruhi model pada saat pelatihan. ini merupakan suatu kendala karena data bad loan (kelas minoritas) dibutuhkan untuk memprediksi model ini


Teknik **oversampling** kelas minoritas akan digunakan untuk mengatasi *imbalance* data ini.

**Oversampling Minority Class to Resolve Class Imbalance**

*Random oversampling* melibatkan duplikasi secara acak contoh dari kelas minoritas dan menambahkannya ke dataset pelatihan.
"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler()
X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)

#periksa jumlah nilai sebelum dan sesudah dilakukan oversampling
print('Before OverSampling:\n{}'.format(y_train.value_counts()))
print('\nAfter OverSampling:\n{}'.format(y_train_ros.value_counts()))

"""**Train the model after over sampling**

Setelah melakukan proses **over sampling** data sudah *balance* / seimbang selanjutnya kita sudah siap untuk melakukan pelatihan pada model dengan mengkomparasikan berbagai algoritma klasifikasi yang ada dan mencari rata-rata akurasi terbaik dari performa yang dihasilkan
"""

#import library for training
from sklearn.linear_model import LogisticRegression #algoritma RegresiLogistik
from sklearn.tree import DecisionTreeClassifier #algoritma Klasifikasi DecisionTree
from sklearn.ensemble import RandomForestClassifier #algoritma Klasifikasi RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier #algoritma Klasifikasi KNeighborsClassifier
from xgboost import XGBClassifier  #algoritma Klasifikasi XGB

"""## 1. Algoritma Logistic Regression"""

# Training 
LR_CF = LogisticRegression(max_iter=600)  
LR_CF.fit(X_train_ros, y_train_ros)

#predicting
y_pred_LR_CF = LR_CF.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_LR_CF, digits=2, target_names = target_names))

"""## 2. Algoritma Klasifikasi Decision Tree"""

# Training 
DT_CF= DecisionTreeClassifier(max_depth = 5)
DT_CF.fit(X_train_ros, y_train_ros)

#predicting
y_pred_DT_CF = DT_CF.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_DT_CF, digits=2, target_names = target_names))

"""## 3. Algoritma Klasifikasi Random Forest"""

# Training 
RF_CF= RandomForestClassifier(max_depth = 5, n_estimators=16)
RF_CF.fit(X_train_ros, y_train_ros)

#predicting
y_pred_RF_CF = RF_CF.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_RF_CF, digits=2, target_names = target_names))

"""## 4. Algoritma Klasifikasi KNeighbors"""

# Training 
KNN_CF= KNeighborsClassifier(n_neighbors=16)
KNN_CF.fit(X_train_ros, y_train_ros)

#predicting
y_pred_KNN_CF = KNN_CF.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_KNN_CF, digits=2, target_names = target_names))

"""## 5. Algoritma Klasifikasi XGB"""

# Training 
XGB_CF= XGBClassifier(max_depth=4)
XGB_CF.fit(X_train_ros, y_train_ros)

#predicting
y_pred_XGB_CF = XGB_CF.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_XGB_CF, digits=2, target_names = target_names))

"""### **KESIMPULAN**

setelah melakukan pelatihan kepada model dengan data yang sudah di **oversampling** , akurasi untuk tiap kelas baik itu *bad loan* ataupun *good loan* didapatkan hasil yang seimbang dan stabil (**rata rata akurasi** untuk tiap kelas **lebih dari (>) 58%**. jadi dapat dikatakan bahwa penggunaan **oversampling** dapat membantu model saat proses pelatihan sehingga bisa mendeteksi kelas-kelas *bad loan* dan *good loan* dengan cukup bagus. 
<br>
<br>
**rata-rata akurasi terbaik** ditempati terhadap semua model yang dihasilkan diatas yaitu dengan menggunakan algoritma klasifikasi *Decision Tree* dimana didapati hasil dengan skor akurasi sebesar **70%** begitu juga nilai recall *bad loan* = **61%** dan nilai recall *good loan* = **71%**. walaupun akurasi yang didapatkan sebesar **70%** yang bisa dibilang kurang begitu tinggi , tapi dengan nilai ini sudah cukup tinggi karena dataset yang *imbalance* kemungkinan yang terjadi sebalik nya bila dataset tidak *imbalance* pasti bisa mencapai akurasi yang lebih tinggi lagi
. oleh karena itu model ini mampu mengidentifikasi 61% dari total *bad loan* dan juga mampu mengidentifikasi 71% dari total *good loan* 
"""